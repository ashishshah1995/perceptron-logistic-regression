# Perceptron Algorithm and Logistic Regression Visualization

This repository contains a Python implementation of a simple perceptron algorithm alongside logistic regression, both used for binary classification

## Perceptron Algorithm

### Overview
The **perceptron algorithm** is one of the simplest algorithms for binary classification. It attempts to find a linear decision boundary (a line in 2D, or a hyperplane in higher dimensions) that separates two classes. The process involves iteratively adjusting the parameters (weights) of the decision boundary based on misclassified points.

### Key Concepts:
- **Random Initialization**: Start with random values for weights (`A`, `B`, and `C` in the line equation `Ax + By + C = 0`).
- **Epochs**: The algorithm loops through the data points for a set number of iterations (in this case, 1000). Each loop is called an **epoch**.
- **Misclassified Points**: For each point, check whether the point is correctly classified by the current decision boundary. If the point is misclassified, update the weights.
- **Point Selection**: A random data point is chosen in each iteration to evaluate whether the decision boundary correctly classifies it.

### Decision Boundary:
The decision boundary is represented by the equation of a line:
```
Ax + By + C = 0
```
- If `Ax + By + C > 0`: The point lies in the positive region.
- If `Ax + By + C < 0`: The point lies in the negative region.
- If `Ax + By + C = 0`: The point lies exactly on the decision boundary.

### Weight Update Rule (Perceptron Trick):
If a point is misclassified:
- **Positive point in the negative region**: Adjust weights to shift the line to classify the point correctly.
- **Negative point in the positive region**: Similarly, adjust weights to push the point into the correct region.
  
The update rule involves changing the weights by adding or subtracting the product of the learning rate and the pointâ€™s coordinates. The learning rate ensures that these adjustments are small, avoiding large transformations.

### Learning Rate:
- The learning rate controls the magnitude of weight updates. A smaller learning rate (0.01 in this case) ensures that the adjustments are gradual, leading to smoother convergence.

---

## Code Breakdown

### Data Generation
I use `make_classification` from `sklearn.datasets` to create a simple dataset for binary classification with two features (2D).
```python
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=100, n_features=2, n_informative=1, n_redundant=0,
                           n_classes=2, n_clusters_per_class=1, random_state=41, hypercube=False, class_sep=10)
```
- **X**: Input data (100 samples, 2 features).
- **y**: Target labels (0 or 1).

### Perceptron Function
The perceptron function updates the weights using the "perceptron trick."
```python
def perceptron(X, y):
    X = np.insert(X, 0, 1, axis=1)  # Insert bias term
    weights = np.ones(X.shape[1])  # Initialize weights
    lr = 0.1  # Learning rate

    for i in range(1000):  # Loop over 1000 epochs
        j = np.random.randint(0, 100)  # Select a random point
        y_hat = step(np.dot(X[j], weights))  # Predict the label using current weights
        weights = weights + lr * (y[j] - y_hat) * X[j]  # Update weights

    return weights[0], weights[1:]
```

### Step Function
The `step` function determines whether the prediction is in the positive or negative region.
```python
def step(z):
    return 1 if z > 0 else 0
```

### Plotting the Decision Boundary
I compute the slope `m` and intercept `b` of the decision boundary:
```python
m = -(coef_[0] / coef_[1])  # Calculate slope from coefficients
b = -(intercept_ / coef_[1])  # Calculate intercept from the intercept term
```
---

## How to Run

1. Install the necessary dependencies:
   ```bash
   pip install numpy matplotlib scikit-learn
   ```
   
2. Clone this repository and run the script:
   ```bash
   git clone https://github.com/ashishshah1995/perceptron-logistic-regression.git
   cd perceptron_logistic_regression
   python perceptron_logistic_regression.py
   ```
---

